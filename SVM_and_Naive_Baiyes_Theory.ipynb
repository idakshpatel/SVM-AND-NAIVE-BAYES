{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UP1dHzv_r4An"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "\n",
        "What is a Support Vector Machine (SVM)?\n",
        "SVM is a supervised machine learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that best separates data points of different classes in the feature space, maximizing the margin between the classes.\n",
        "\n",
        "What is the difference between Hard Margin and Soft Margin SVM?\n",
        "\n",
        "Hard Margin SVM strictly separates the data with no misclassifications, suitable for linearly separable data.\n",
        "Soft Margin SVM allows some misclassifications using a penalty parameter C to balance margin width and classification errors, making it suitable for non-linearly separable data.\n",
        "\n",
        "what is the mathematical intuition behind SVM?\n",
        "\n",
        "\n",
        "The mathematical intuition behind Support Vector Machine (SVM) lies in finding the optimal\n",
        "hyperplane that best separates the data points of different classes while maximizing the margin (the\n",
        "distance between the hyperplane and the nearest data points from each class, known as support\n",
        "vectors)\n",
        "Objective: Maximize the Margin\n",
        "The goal of SVM is to find a hyperplane defined by:\n",
        "—o\n",
        "Where:\n",
        "W = weight vector (defines the orientation of the hyperplane)\n",
        "b = bias (shifts the hyperplane)\n",
        "a: = input feature vector\n",
        "2\n",
        "To maximize it, we need to minimize WII.\n",
        "The margin is defined as\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "What is the role of Lagrange Multipliers in SVM?\n",
        "Lagrange Multipliers help solve the constrained optimization problem in SVM by converting it into a dual form, making it easier to handle non-linearly separable data and incorporate kernel functions.\n",
        "\n",
        "What are Support Vectors in SVM?\n",
        "Support Vectors are the data points closest to the hyperplane. They influence the position and orientation of the hyperplane, and the margin depends solely on them.\n",
        "\n",
        "What is a Support Vector Classifier (SVC)?\n",
        "SVC is an implementation of SVM for classification tasks. It uses kernel functions to handle non-linear data and finds the optimal separating hyperplane.\n",
        "\n",
        "What is a Support Vector Regressor (SVR)?\n",
        "SVR applies the SVM framework to regression tasks. It tries to fit the best line within a margin of tolerance (epsilon) while penalizing points outside this margin.\n",
        "\n",
        "What is the Kernel Trick in SVM?\n",
        "The Kernel Trick maps data into higher-dimensional space without explicitly computing the transformation, enabling SVM to handle non-linear data using functions like RBF, Polynomial, and Sigmoid.\n",
        "Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n",
        "\n",
        "Linear Kernel: Best for linearly separable data; simple and efficient.\n",
        "Polynomial Kernel: Captures complex relationships by adding polynomial terms.\n",
        "RBF Kernel: Handles non-linear data by mapping into infinite-dimensional space; sensitive to parameters.\n",
        "What is the effect of the C parameter in SVM?\n",
        "\n",
        "High C → Focuses on classifying all points correctly (low bias, high variance).\n",
        "Low C → Allows more misclassifications for a wider margin (high bias, low variance).\n",
        "What is the role of the Gamma parameter in RBF Kernel SVM?\n",
        "Gamma defines the influence of a single training example:\n",
        "\n",
        "High Gamma → Closer data points have more influence (may overfit).\n",
        "Low Gamma → Points farther apart affect the decision boundary (may underfit).\n",
        "Naive Bayes Related Questions\n",
        "What is the Naive Bayes classifier, and why is it called \"Naive\"?\n",
        "Naive Bayes is a probabilistic classifier based on Bayes' Theorem, assuming independence between features. It’s called \"naive\" because it assumes that all features are independent given the class, which is rarely true in real data.\n",
        "\n",
        "What is Bayes' Theorem?\n",
        "\n",
        "\n",
        "Bayes' Theorem is a fundamental concept in probability theory and statistics that describes the relationship between conditional probabilities.\n",
        "It allows the calculation of the probability of an event based on prior knowledge of related events.\n",
        "It calculates the probability of event A occurring given that B has occurred.\n",
        "\n",
        "Explain the differences between Gaussian Naive Bayes, Multinomial Naive Bayes, and Bernoulli Naive Bayes.\n",
        "\n",
        "Gaussian Naive Bayes: Assumes features follow a normal distribution (useful for continuous data).\n",
        "Multinomial Naive Bayes: Used for discrete count data (e.g., word counts in text classification).\n",
        "Bernoulli Naive Bayes: Assumes binary features (e.g., word presence/absence).\n",
        "When should you use Gaussian Naive Bayes over other variants?\n",
        "Use Gaussian Naive Bayes when the features are continuous and follow a normal distribution, like in medical or sensor data.\n",
        "\n",
        "What are the key assumptions made by Naive Bayes?\n",
        "\n",
        "Feature independence given the class label.\n",
        "Equal contribution of each feature to the outcome.\n",
        "Conditional probability distributions of features are correctly modeled.\n",
        "What are the advantages and disadvantages of Naive Bayes?\n",
        "Advantages:\n",
        "\n",
        "Simple, fast, and efficient.\n",
        "Works well with high-dimensional data.\n",
        "Performs well even with small datasets.\n",
        "Disadvantages:\n",
        "Strong independence assumption may not hold true.\n",
        "Poor performance with highly correlated features.\n",
        "Why is Naive Bayes a good choice for text classification?\n",
        "\n",
        "Efficient with high-dimensional sparse data (e.g., text).\n",
        "The independence assumption fits well with bag-of-words models.\n",
        "Performs well even with limited training data.\n",
        "Compare SVM and Naive Bayes for classification tasks.\n",
        "\n",
        "Aspect\tSVM\tNaive Bayes\n",
        "Complexity\tComputationally expensive\tSimple and fast\n",
        "Performance\tOften better on complex datasets\tStrong with text and categorical data\n",
        "Assumptions\tNo probabilistic assumptions\tAssumes feature independence\n",
        "Use Cases\tImage classification, bioinformatics\tText classification, spam detection\n",
        "How does Laplace Smoothing help in Naive Bayes?\n",
        "Laplace Smoothing prevents zero probabilities by adding a small constant (usually 1) to all counts.\n",
        " This ensures that unseen features in the test data don’t nullify the probability calculation.\n",
        "\n",
        " '''"
      ]
    }
  ]
}